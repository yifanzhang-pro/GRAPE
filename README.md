# GRAPE: Group Representational Position Encoding

[![arXiv](https://img.shields.io/badge/arXiv-2512.07805-b31b1b.svg)](https://arxiv.org/abs/2512.07805)
[![License](https://img.shields.io/badge/License-Apache_2.0-yellow.svg)](https://opensource.org/licenses/Apache-2.0)
[![Python 3.12+](https://img.shields.io/badge/python-3.12+-blue.svg)](https://www.python.org/downloads/)

This repository contains the official implementation of the paper **"Group Representational Position Encoding (GRAPE)"**.

GRAPE is a unified group-theoretic framework for positional encoding that subsumes multiplicative mechanisms (like RoPE) and additive mechanisms (like ALiBi and FoX) under a single mathematical formalism. By leveraging group actions, specifically rotations in $SO(d)$ and unipotent lifts in $GL(d+k)$, GRAPE guarantees exact relative position laws and efficient streaming cacheability.

Authors: [Yifan Zhang](https://yifzhang.com), [Zixiang Chen](https://sites.google.com/view/zxchen/home), [Yifeng Liu](https://lauyikfung.github.io), [Zhen Qin](https://doraemonzzz.com), [Huizhuo Yuan](https://scholar.google.com/citations?user=8foZzX4AAAAJ), [Kangping Xu](), [Yang Yuan](https://scholar.google.com/citations?user=7o4wtKEAAAAJ&hl=en), [Quanquan Gu](https://web.cs.ucla.edu/~qgu/), [Andrew Chi-Chih Yao](https://en.wikipedia.org/wiki/Andrew_Yao) 

[[Webpage](https://tensorgi.github.io/TPA)] 

## üìñ Abstract

Positional information is essential for sequence modeling with Transformers. We present **GRAPE**, a framework that unifies two families of mechanisms:

1.  **Multiplicative GRAPE ($SO(d)$):** Models positions as norm-preserving rotations generated by rank-2 skew-symmetric matrices. It recovers **RoPE** exactly when the planes are canonical coordinate pairs with a log-uniform spectrum. It further extends to learned commuting subspaces and compact non-commuting mixtures.
2.  **Additive GRAPE ($GL(d+k)$):** Models positions as unipotent actions in a lifted homogeneous space. It recovers **ALiBi** and the **Forgetting Transformer (FoX)** as exact special cases while preserving an exact relative law and streaming cacheability.

## üöÄ Key Methodologies

### 1. Multiplicative GRAPE (The $SO(d)$ Action)

Positions $n \in \mathbb{Z}$ act on the query/key space via a matrix exponential of a generator $L$:
$$G(n) = \exp(n \omega L) \in SO(d)$$
where $L = ab^\top - ba^\top$ is a rank-2 skew-symmetric generator.

We derive a closed-form **Rodrigues-type formula** for fast $\mathcal{O}(d)$ application without materializing the matrix exponential:
$$\exp(L) = I + \frac{\sin s}{s}L + \frac{1 - \cos s}{s^2}L^2$$
where $s$ is the scaling factor determined by the generator vectors $a$ and $b$.

### 2. Additive GRAPE (The Unipotent Lift)

Additive biases are realized via a homogeneous lift to $GL(d+k)$ using a nilpotent generator $A$ (where $A^2=0$). The group action is defined as:
$$G_{add}(n) = \exp(n \omega A) = I + n \omega A$$
This yields an exact relative law where attention scores depend strictly on the offset $j-i$:
$$\tilde{q}_i^\top \tilde{k}_j = q_i^\top k_j + \text{bias}(j-i)$$

## üõ†Ô∏è Installation

```bash
conda create -n grape python=3.12
conda activate grape
pip install torch==2.4.0 numpy transformers datasets tiktoken wandb tqdm
````

## üíª Usage

### Multiplicative GRAPE (RoPE-style)

```python
import torch

def apply_grape_m(q, k, frequencies):
    """
    Applies Multiplicative GRAPE (Rotary) using the closed-form 
    rank-2 update logic described in Section 2.3.
    """
    # Assuming q, k are shape (B, Seq, Heads, Dim)
    # Frequencies shape (Dim/2)
    
    # 1. Reshape for rank-2 plane operations
    q_reshaped = q.float().reshape(*q.shape[:-1], -1, 2)
    k_reshaped = k.float().reshape(*k.shape[:-1], -1, 2)
    
    # 2. Compute sin/cos tables (standard caching applies here)
    # Note: In GRAPE, 'theta' corresponds to n * omega
    # Implementation details match Algorithm 1 in Appendix G
    ...
    
    # 3. Apply rotation (standard complex number multiplication view)
    # This recovers the standard RoPE implementation when bases are canonical.
    q_out = complex_mult(q_reshaped, freqs_cis)
    k_out = complex_mult(k_reshaped, freqs_cis)
    
    return q_out, k_out
```

### Additive GRAPE (ALiBi/FoX-style)

```python
import torch

def compute_grape_a_bias(seq_len, num_heads, slope_rate):
    """
    Computes the Additive GRAPE bias matrix.
    Equivalent to ALiBi when using the canonical rank-1 unipotent generator.
    """
    # Generate relative positions
    context_position = torch.arange(seq_len)[:, None]
    memory_position = torch.arange(seq_len)[None, :]
    relative_position = memory_position - context_position 
    
    # Apply scalar slope (omega * u^T * v)
    # Section 4.1: Linear-in-offset bias
    bias = relative_position * slope_rate
    
    # Mask future positions
    mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1).bool()
    bias.masked_fill_(mask, float('-inf'))
    
    return bias
```

## üìä Experiments

We evaluate GRAPE on language modeling tasks using the **FineWeb-Edu 100B** dataset. GRAPE-A (Additive) and GRAPE-M (Multiplicative) consistently outperform or match baselines like RoPE, ALiBi, and FoX in perplexity and downstream tasks.

| Method | Avg. Score (Medium Model) | Avg. Score (Large Model) |
| :--- | :---: | :---: |
| RoPE | 53.44 | 56.16 |
| ALiBi | 54.09 | 57.19 |
| FoX | 51.96 | 55.44 |
| **GRAPE-A** | **54.54** | **57.25** |

*See Tables 1 and 2 in the paper for full breakdown.*

## üìù Citation

If you find this work useful, please cite our paper:

```bibtex
@article{zhang2025grape,
  title={Group Representational Position Encoding},
  author={Zhang, Yifan and Xu, Kangping and Chen, Zixiang and Liu, Yifeng and Qin, Zhen and Yuan, Huizhuo and Gu, Quanquan and Yuan, Yang and Yao, Andrew Chi-Chih},
  journal={arXiv preprint arXiv:2512.07805},
  year={2025}
}
```

## üìú License

This project is licensed under the [Apache License 2.0](https://www.google.com/search?q=LICENSE).
